# AI Development Effectiveness Tracker

## Quick Start

1. **Open in Google Sheets**: Upload `ai-development-tracker.csv`
2. **Each week**: Add rows for tasks you complete
3. **End of week**: Review your metrics

## How to Fill Out

### Basic Info
- **Developer**: Your name
- **Week**: Week number (1-6 from workshop)
- **Date Start/End**: Week start and end dates
- **Task ID**: From `documents/03-dev-tasks/` (e.g., TASK-012)
- **Task Name**: Brief task title
- **Status**: `Done`, `In Progress`, or `Blocked`

### Time Tracking
- **Estimated Hours**: Original estimate from task card
- **Actual Hours**: How long it actually took
- **Time Saved %**: Auto-calculate: `((Estimated - Actual) / Estimated) Ã— 100`

### AI Usage
- **AI Prompts Used**: Count of prompts/commands you used
- **Code Generated by AI %**: Rough estimate (0-100%)
  - 80%+ = Agent wrote most code
  - 50-80% = Mix of AI and manual
  - 0-50% = Mostly manual with AI help

### Effectiveness Scores (1-5 scale)
- **Quality Score**: How good was AI-generated code?
  - 5 = Perfect, production-ready
  - 4 = Minor tweaks needed
  - 3 = Significant edits required
  - 2 = Major rework needed
  - 1 = Had to rewrite from scratch

- **Time Saved Score**: Did AI save you time?
  - 5 = Saved 40%+ time
  - 4 = Saved 25-40% time
  - 3 = Saved 10-25% time
  - 2 = Broke even
  - 1 = Slower than manual

- **Learning Score**: Did you learn something?
  - 5 = Learned major new concepts
  - 4 = Learned useful patterns
  - 3 = Learned a few tricks
  - 2 = Minor learnings
  - 1 = Nothing new

### Quality Metrics
- **Bugs Found**: Issues discovered during development
- **Bugs Fixed**: Issues you resolved
- **Agents Used**: Which agents (e.g., `frontend-ui`, `backend-api`)
- **MCP Tools Used**: Which MCP servers (e.g., `Playwright`, `Context7`, `GitHub`)

### Context
- **Blockers**: What slowed you down? (e.g., "Dependency not ready", "Unclear requirements")
- **Wins**: What went really well?
- **Notes**: Any observations, learnings, or context

## Example Entry

```csv
John Doe,1,2024-01-15,2024-01-19,TASK-012,Build login page UI,Done,6,3.2,46.7,12,70,5,5,4,1,1,frontend-ui qa-tester,Context7 Playwright,None,Caught bug with Playwright MCP,React 19 patterns from Context7 saved time
```

**Translation**:
- John completed TASK-012 in Week 1
- Estimated 6 hours, took 3.2 hours (saved 46.7%)
- Used 12 AI prompts, AI generated ~70% of code
- All scores high (5/5 quality, 5/5 time saved, 4/5 learning)
- Found and fixed 1 bug during development
- Used `frontend-ui` and `qa-tester` agents
- Used Context7 and Playwright MCP tools
- No blockers
- Win: Caught bug early with Playwright MCP
- Note: Context7 provided latest React 19 patterns

## Weekly Summary Template

At end of each week, calculate:

### Personal Metrics
```
Developer: [Your Name]
Week: [1-6]

Total Hours: ___h
Tasks Completed: ___ / ___ planned
Average Time Saved: ___%
AI Prompts Used: ___
Average Quality Score: ___/5
Average Time Saved Score: ___/5
Average Learning Score: ___/5
Bugs Found/Fixed: ___ / ___
```

### Reflection
```
Top Wins:
- [What went really well?]
- [What surprised you?]

Key Learnings:
- [What did you learn about AI-assisted development?]
- [What patterns/techniques were most effective?]

Challenges:
- [What was difficult?]
- [Where did AI struggle?]

Next Week:
- [What will you try differently?]
- [What skills do you want to develop?]
```

## Team Comparison

Compare metrics across team:

| Metric | With AI | Without AI | Difference |
|--------|---------|------------|------------|
| Avg Time per Task | ___ | ___ | ___% |
| Avg Quality Score | ___/5 | ___/5 | ___ |
| Bugs in Production | ___ | ___ | ___ |
| Tasks Completed | ___ | ___ | ___ |

## Tips

1. **Be honest**: Track actual time, not idealized time
2. **Track prompts**: Keep count of how many times you prompt agents
3. **Note context**: "Why" you saved/lost time is as important as "how much"
4. **Compare weekly**: Look for trends in your effectiveness
5. **Share learnings**: Best practices emerge from comparing notes

## Key Questions to Answer

After 3-4 weeks of data:

- **Is AI saving time?** Look at "Time Saved %" across all tasks
- **Where does AI excel?** Compare scores by task type (Frontend vs Backend vs Database)
- **Where does AI struggle?** Low quality scores indicate areas needing human expertise
- **Are you learning?** Learning score should stay high (3+)
- **Quality trade-offs?** Compare bugs found AI vs non-AI tasks

## Red Flags

Watch for these patterns:

- **Negative time savings**: AI might not be right approach for this task type
- **Low quality scores (< 3)**: Need better prompts or different agent
- **High rework**: Acceptance criteria might be unclear
- **Many blockers**: Dependencies or requirements need clarification

## Success Patterns

Good indicators:

- **Consistent 25-50% time savings**: Realistic AI benefit
- **Quality scores 4-5**: AI generating production-ready code
- **High learning scores**: You're growing, not just copying code
- **Decreasing prompts over time**: You're learning effective prompting
- **Wins > Blockers**: Momentum is positive

---

**Goal**: By Week 6, you should have clear data on:
1. Where AI helps most (task types, patterns)
2. Where human expertise is essential
3. Your personal AI-assisted development velocity
4. Team best practices and patterns
